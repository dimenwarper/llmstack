# LLM Training Stack - Context

## Project Overview
This project creates a complete LLM training pipeline using only agentic tools (Claude Code, Cursor) with minimal manual editing. The goal is to demonstrate end-to-end model development through AI-assisted coding.

## Project Structure
```
llmstack/
├── models/        # Model architectures and configurations
├── pretraining/   # Initial model training from scratch
├── midtraining/   # Intermediate training and fine-tuning
├── posttraining/  # Final training phase and optimization
├── references/    # Research papers and helpful references
├── CONTEXT.md     # Project context and requirements
└── README.md      # Project overview
```

## Reference Materials
The `references/` directory contains research papers, technical documentation, and other materials that inform the implementation of each training phase. These resources guide architectural decisions and best practices throughout the development process.

## Training Pipeline Stages

### 1. Pretraining
- **Purpose**: Train foundation model from scratch
- **Data**: Large-scale text corpora
- **Techniques**: Unsupervised learning, next-token prediction
- **Output**: Base language model

### 2. Midtraining  
- **Purpose**: Domain-specific adaptation and fine-tuning
- **Data**: Curated domain-specific datasets
- **Techniques**: Supervised fine-tuning, task-specific training
- **Output**: Specialized model for target domain

### 3. Posttraining
- **Purpose**: Final optimization and alignment
- **Data**: Human feedback, safety datasets
- **Techniques**: RLHF, constitutional AI, safety training
- **Output**: Production-ready aligned model

## Technical Requirements
- Framework: PyTorch/Transformers (TBD)
- Hardware: GPU cluster support
- Data handling: Efficient data loading and preprocessing
- Monitoring: Training metrics and logging
- Checkpointing: Model saving and resumption

## Development Constraints
- All code written by AI assistants
- Minimal manual editing allowed
- Focus on clean, maintainable architecture
- Comprehensive documentation generated by AI

## Success Metrics
- Functional training pipeline for each phase
- Clear separation of concerns between stages
- Scalable and configurable architecture
- Complete documentation and examples