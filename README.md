# LLM Training Stack

A complete LLM training pipeline built entirely with agentic tools (Claude Code, Cursor) with minimal manual editing.

## Overview

This project demonstrates a full machine learning lifecycle for large language models, spanning from initial pretraining through midtraining to final posttraining phases. The unique constraint is that all development must be done using AI coding assistants with little to no manual code editing.

## Project Structure

```
llmstack/
├── models/        # Model architectures and configurations
├── pretraining/   # Initial model training from scratch
├── midtraining/   # Intermediate training and fine-tuning
├── posttraining/  # Final training phase and optimization
└── README.md
```

## Training Phases

### Pretraining
- Foundation model training from scratch
- Large-scale unsupervised learning
- Base model architecture implementation

### Midtraining
- Domain-specific fine-tuning
- Intermediate model refinement
- Performance optimization

### Posttraining
- Final model polishing
- Alignment and safety training
- Production-ready model preparation

## Development Methodology

This project serves as an experiment in agentic development, where AI coding assistants handle the majority of implementation work with minimal human intervention in the actual coding process.